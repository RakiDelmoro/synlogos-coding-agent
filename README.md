![Synlogos CLI](cli-display.png)

A professional multi-provider AI coding agent built with functional programming patterns in Python. Synlogos acts as your autonomous coding companion, supporting multiple LLM providers and specialized agent types for different coding tasks.

## Features

- **Multi-Provider Support** â€” Works with opencode.ai (free), TogetherAI, Ollama (local), Groq, and any OpenAI-compatible API
- **Specialized Agent Types** â€” Different agents optimized for specific tasks:
  - `explore` â€” Fast file and codebase exploration
  - `code` â€” Primary coding agent for complex tasks
  - `architect` â€” System design and architecture decisions
  - `plan` â€” Task planning and analysis
  - `grep` â€” Search specialist
  - `summarize` â€” Code and diff summarization
  - `web_search` â€” Web information retrieval
  - `memory` â€” Memory management agent
- **Hybrid Tool Calling** â€” Intelligent routing between direct tools and programmatic orchestration
  - Simple tasks use direct tools (faster, fewer tokens)
  - Complex tasks use orchestration (parallel execution, batch operations)
- **Auto-Compact** â€” NEW: Automatically summarizes conversation at 80% of context limit (OpenCode-style)
  - Prevents token bloat and context window overflow
  - Keeps only recent messages + summary
- **Session Metrics** â€” Real-time tracking of tool usage and efficiency
  - `/metrics` command shows hybrid ratio and success rates
- **JSON Configuration** â€” Simple `synlogos.json` config file for providers and agents
- **Programmatic Tool Calling** â€” LLM writes code that orchestrates multiple tools efficiently
- **Functional Architecture** â€” Built with Result monads, immutable state, and pure functions
- **Rich Tool Set**:
  - File operations: read, write, edit
  - Shell command execution
  - Code execution in sandboxed environment
  - File search: glob patterns, grep/regex search
  - Git integration: status, diff, log, commit
- **Beautiful CLI** â€” Rich terminal output with markdown rendering
- **Safe Execution** â€” Local sandbox for isolated code execution

### Hybrid Tool Calling

Synlogos now uses an intelligent **hybrid approach** that automatically chooses the best execution method:

**Direct Tools** (Single operations):
- `read_file`, `write_file`, `edit_file` â€” File operations
- `shell` â€” Shell command execution
- `execute_code` â€” Temporary code execution
- `glob`, `grep` â€” File and content search
- `git_*` â€” Git operations

**Orchestration** (Complex/multi-step):
- Write Python code that calls multiple tools
- Parallel execution via `asyncio.gather()`
- Batch operations and complex logic
- Process intermediate results in code

**Benefits:**
- âœ… Simple tasks use fewer tokens (~43% reduction)
- âœ… Complex tasks still get full orchestration power
- âœ… Real-time metrics show which mode is used

## Installation

```bash
# Clone the repository
git clone https://github.com/RakiDelmoro/synlogos-coding-agent.git
cd synlogos-coding-agent

# Install with pip
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

## Configuration

Synlogos uses `synlogos.json` for configuration. Create this file in your project root:

```json
{
  "$schema": "https://opencode.ai/config.json",
  "theme": "matrix",
  "instructions": ["SOUL.md"],
  "provider": {
    "opencode": {
      "npm": "@ai-sdk/openai-compatible",
      "options": {
        "baseURL": "https://opencode.ai/zen/v1",
        "apiKey": "your-api-key"
      },
      "models": {
        "glm-5-free": { "model": "glm-5-free" },
        "kimi-k2-free": { "model": "kimi-k2-free" }
      }
    },
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "qwen3:8b": { "model": "qwen3:8b" }
      }
    },
    "togetherai": {
      "npm": "@ai-sdk/openai-compatible",
      "options": {
        "baseURL": "https://api.together.xyz/v1",
        "apiKey": "your-api-key"
      },
      "models": {
        "moonshotai/Kimi-K2.5": {}
      }
    }
  },
  "model": "togetherai/moonshotai/Kimi-K2.5",
  "agent": {
    "explore": {
      "model": "ollama/qwen3:8b",
      "instructions": "You are a fast file and codebase explorer..."
    },
    "code": {
      "model": "togetherai/moonshotai/Kimi-K2.5",
      "instructions": "You are the primary coding agent..."
    }
  }
}
```

### Provider Setup

**Free Options:**
- **opencode.ai** â€” Get free API key at https://opencode.ai (no signup required)
- **Ollama** â€” Run locally: `ollama pull qwen3:8b && ollama serve`

**Commercial Options:**
- **TogetherAI** â€” Get API key at https://api.together.xyz/settings/api-keys
- **Groq** â€” Get free tier API key at https://console.groq.com/keys

## Quick Start

1. Create your `synlogos.json` config file (see example above)

2. Run Synlogos:

```bash
# Default agent
synlogos

# Or use a specific agent type
synlogos --agent explore
synlogos --agent code
synlogos --agent architect

# See available agents
synlogos --list-agents

# See configured providers
synlogos --list-providers
```

## Usage

Once started, you'll see an interactive prompt. Just describe what you want to do:

```
You: Create a Python function that calculates fibonacci numbers and write it to fib.py

You: Read all the TypeScript files in src/ and summarize their purpose

You: Run the tests and fix any failing tests

You: Commit the changes with a descriptive message
```

Type `exit` or `quit` to end the session.

### Slash Commands

While the agent is running, you can use slash commands for quick actions:

```
/help              Show available slash commands
/agents            List all available agent types
/provider          Show current provider and model
/providers         List all configured providers
/tokens            Show current token usage
/metrics           Show session tool usage metrics (NEW)
/config            Show current configuration
clear              Clear the screen
/exit              Exit the session (same as 'exit')
```

Example:
```
You: /help
You: /agents
You: /tokens
You: /metrics
You: /provider
```

**Session Metrics (`/metrics`):**
```
============================================================
SESSION METRICS
============================================================
Session duration: 0:00:05.123456
Total prompts: 5
Direct tool calls: 7
Orchestration calls: 2

Tool Usage Breakdown:
------------------------------------------------------------
  write_file             3 calls  100.0% success
  read_file              2 calls  100.0% success
  orchestrate            2 calls  100.0% success
------------------------------------------------------------
Hybrid ratio: 7:2 (direct:orchestrate)
============================================================
```

### Real-Time Token Usage

Token usage is displayed in real-time as you interact with the agent:

```
You: What is 2+2?

ðŸ¤” Thinking
The result of 2+2 is 4.

âœ… Final Result
4

Tokens: 286 (â†‘2,526 â†“160)
```

The token counter shows:
- **Total tokens used** in the current session
- **â†‘ Prompt tokens** sent to the API  
- **â†“ Completion tokens** received from the API

This updates after each API call, helping you track costs and context window usage in real-time.

### CLI Options

```
synlogos [options]

Options:
  --agent TYPE          Agent type: explore, code, architect, plan, grep, summarize, web_search, memory
  --list-agents         Show available agents and exit
  --list-providers      Show configured providers and exit
  --config PATH         Path to synlogos.json config file
  --max-turns N         Maximum conversation turns (default: 30)
  -h, --help            Show help message
```

## Architecture

Synlogos is built with functional programming principles:

- **Result Monads** â€” Explicit error handling using the `returns` library
- **Immutable State** â€” State is threaded through pure functions, never mutated
- **Multi-Provider Design** â€” Unified OpenAI-compatible API wrapper for all providers
- **JSON-Based Config** â€” Simple declarative configuration
- **Specialized Agents** â€” Each agent type has optimized models and instructions

```
src/
â”œâ”€â”€ agent/              # Agent implementations
â”‚   â””â”€â”€ synlogos.py     # Main Synlogos agent with multi-provider support
â”œâ”€â”€ providers/          # LLM providers (unified interface)
â”‚   â”œâ”€â”€ unified_provider.py  # Universal OpenAI-compatible provider
â”‚   â”œâ”€â”€ groq_provider.py     # Groq provider
â”‚   â””â”€â”€ ollama_provider.py   # Ollama provider
â”œâ”€â”€ config.py           # JSON config loader
â”œâ”€â”€ sandbox/            # Code execution sandbox
â”œâ”€â”€ tools/              # Tool implementations
â”‚   â”œâ”€â”€ functional_tools.py  # File ops, shell, code exec
â”‚   â”œâ”€â”€ advanced_tools.py    # Glob, grep
â”‚   â””â”€â”€ git_tools.py         # Git operations
â”œâ”€â”€ types.py            # Type definitions
â””â”€â”€ cli.py              # CLI entry point
```

## Recent Improvements

### v2.0 - Hybrid Tool Calling & Session Metrics

**ðŸŽ¯ Hybrid Mode**
- Intelligent routing between direct tools and orchestration
- Simple tasks: ~43% token reduction
- Complex tasks: Full orchestration power maintained
- Automatic selection based on task complexity

**ðŸ”„ Auto-Compact**
- OpenCode-style automatic conversation summarization
- Triggers at 2K tokens (50% of 4K context)
- Keeps last 3 messages + summary when compacting
- Prevents 64K+ token bloat issues
- System prompt reduced by 85% (~2K tokens saved per call)

**ðŸ“Š Session Metrics**
- Real-time tracking with `/metrics` command
- Tool usage breakdown (direct vs orchestrate)
- Success rates per tool
- Session efficiency analysis

**âœ¨ Quality Improvements**
- Simplified system prompts (70% shorter)
- Better multi-line string handling in code
- Improved JSON parsing for tool arguments
- Enhanced error handling

## How It Works

1. **Configuration** â€” Load `synlogos.json` to determine provider and model
2. **Agent Selection** â€” Choose agent type (each has its own model and instructions)
3. **Prompt Processing** â€” User prompt sent to selected LLM
4. **Tool Selection** â€” LLM chooses: direct tool call OR orchestration
5. **Execution** â€” Tools execute operations (direct) or code runs in sandbox (orchestrate)
6. **Results** â€” Final results returned with metrics tracking

## Requirements

- Python 3.11+
- API key for your chosen provider (or run Ollama locally)
- Docker (optional, for sandboxed code execution)

## Dependencies

- `openai` â€” OpenAI SDK (for OpenAI-compatible APIs)
- `pydantic` â€” Data validation
- `returns` â€” Result monads for functional error handling
- `rich` â€” Beautiful terminal output
- `aiofiles` â€” Async file operations
- `httpx` â€” HTTP client

## Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run linting
ruff check .

# Run type checking
mypy src/

# Run tests
pytest
```

## Migrating from v1

If you were using Synlogos v1 (TogetherAI-only):

1. Update your environment variables:
   - Remove: `TOGETHER_API_KEY`
   - Add: Create `synlogos.json` config file

2. Update imports (if using programmatically):
   ```python
   # Old
   from src.types import AgentConfig
   config = AgentConfig(model="meta-llama/...")
   
   # New
   from src.agent.synlogos import Synlogos
   agent = Synlogos(agent_type="code")
   ```

## License

MIT License â€” see [LICENSE](LICENSE) for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Acknowledgments

Built with love using:
- [OpenCode](https://opencode.ai) â€” Free AI model access
- [TogetherAI](https://together.ai) â€” Fast, affordable LLM inference
- [Ollama](https://ollama.ai) â€” Local LLM inference
- [Groq](https://groq.com) â€” Fast inference
- [Rich](https://github.com/Textualize/rich) â€” Beautiful terminal formatting
- [Returns](https://github.com/dry-python/returns) â€” Functional programming in Python
